{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!gdown https://drive.google.com/uc?id=1DdGIJ4PZPlF2ikl8mNM9V-PdVxVLbQi6\\n!unzip -q lol_dataset.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The LoL Dataset has been created for low-light image enhancement. It provides 485 images for training and 15 for testing.\n",
    "Each image pair in the dataset consists of a low-light input image and its corresponding well-exposed reference image.\n",
    "\n",
    "https://keras.io/examples/vision/mirnet/\"\"\"\n",
    "\n",
    "\"\"\"!gdown https://drive.google.com/uc?id=1DdGIJ4PZPlF2ikl8mNM9V-PdVxVLbQi6\n",
    "!unzip -q lol_dataset.zip\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 4\n",
    "MAX_TRAIN_IMAGES = 300\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_crop(low_image, enhanced_image):\n",
    "    low_image_shape = tf.shape(low_image)[:2]\n",
    "    low_w = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[1] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    low_h = tf.random.uniform(\n",
    "        shape=(), maxval=low_image_shape[0] - IMAGE_SIZE + 1, dtype=tf.int32\n",
    "    )\n",
    "    enhanced_w = low_w\n",
    "    enhanced_h = low_h\n",
    "    low_image_cropped = low_image[\n",
    "        low_h : low_h + IMAGE_SIZE, low_w : low_w + IMAGE_SIZE\n",
    "    ]\n",
    "    enhanced_image_cropped = enhanced_image[\n",
    "        enhanced_h : enhanced_h + IMAGE_SIZE, enhanced_w : enhanced_w + IMAGE_SIZE\n",
    "    ]\n",
    "    return low_image_cropped, enhanced_image_cropped\n",
    "\n",
    "\n",
    "def load_data(low_light_image_path, enhanced_image_path):\n",
    "    low_light_image = read_image(low_light_image_path)\n",
    "    enhanced_image = read_image(enhanced_image_path)\n",
    "    low_light_image, enhanced_image = random_crop(low_light_image, enhanced_image)\n",
    "    return low_light_image, enhanced_image\n",
    "\n",
    "\n",
    "def get_dataset(low_light_images, enhanced_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "# loading the temp.zip and creating a zip object\n",
    "with ZipFile(\"C:\\\\Users\\\\seeho\\\\lol_dataset.zip\", 'r') as zObject:\n",
    "        zObject.extractall(path=\"C:\\\\Users\\\\seeho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: <BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n",
      "Val Dataset: <BatchDataset element_spec=(TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(4, None, None, 3), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
    "train_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[:MAX_TRAIN_IMAGES]\n",
    "\n",
    "val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
    "val_enhanced_images = sorted(glob(\"./lol_dataset/our485/high/*\"))[MAX_TRAIN_IMAGES:]\n",
    "\n",
    "test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
    "test_enhanced_images = sorted(glob(\"./lol_dataset/eval15/high/*\"))\n",
    "\n",
    "\n",
    "train_dataset = get_dataset(train_low_light_images, train_enhanced_images)\n",
    "val_dataset = get_dataset(val_low_light_images, val_enhanced_images)\n",
    "\n",
    "\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_kernel_feature_fusion(\n",
    "    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3\n",
    "):\n",
    "    channels = list(multi_scale_feature_1.shape)[-1]\n",
    "    combined_feature = layers.Add()(\n",
    "        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]\n",
    "    )\n",
    "    gap = layers.GlobalAveragePooling2D()(combined_feature)\n",
    "    channel_wise_statistics = tf.reshape(gap, shape=(-1, 1, 1, channels))\n",
    "    compact_feature_representation = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(channel_wise_statistics)\n",
    "    feature_descriptor_1 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_2 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_descriptor_3 = layers.Conv2D(\n",
    "        channels, kernel_size=(1, 1), activation=\"softmax\"\n",
    "    )(compact_feature_representation)\n",
    "    feature_1 = multi_scale_feature_1 * feature_descriptor_1\n",
    "    feature_2 = multi_scale_feature_2 * feature_descriptor_2\n",
    "    feature_3 = multi_scale_feature_3 * feature_descriptor_3\n",
    "    aggregated_feature = layers.Add()([feature_1, feature_2, feature_3])\n",
    "    return aggregated_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention_block(input_tensor):\n",
    "    average_pooling = tf.reduce_max(input_tensor, axis=-1)\n",
    "    average_pooling = tf.expand_dims(average_pooling, axis=-1)\n",
    "    max_pooling = tf.reduce_mean(input_tensor, axis=-1)\n",
    "    max_pooling = tf.expand_dims(max_pooling, axis=-1)\n",
    "    concatenated = layers.Concatenate(axis=-1)([average_pooling, max_pooling])\n",
    "    feature_map = layers.Conv2D(1, kernel_size=(1, 1))(concatenated)\n",
    "    feature_map = tf.nn.sigmoid(feature_map)\n",
    "    return input_tensor * feature_map\n",
    "\n",
    "\n",
    "def channel_attention_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    average_pooling = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "    feature_descriptor = tf.reshape(average_pooling, shape=(-1, 1, 1, channels))\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels // 8, kernel_size=(1, 1), activation=\"relu\"\n",
    "    )(feature_descriptor)\n",
    "    feature_activations = layers.Conv2D(\n",
    "        filters=channels, kernel_size=(1, 1), activation=\"sigmoid\"\n",
    "    )(feature_activations)\n",
    "    return input_tensor * feature_activations\n",
    "\n",
    "\n",
    "def dual_attention_unit_block(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    feature_map = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(input_tensor)\n",
    "    feature_map = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(\n",
    "        feature_map\n",
    "    )\n",
    "    channel_attention = channel_attention_block(feature_map)\n",
    "    spatial_attention = spatial_attention_block(feature_map)\n",
    "    concatenation = layers.Concatenate(axis=-1)([channel_attention, spatial_attention])\n",
    "    concatenation = layers.Conv2D(channels, kernel_size=(1, 1))(concatenation)\n",
    "    return layers.Add()([input_tensor, concatenation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Residual Modules\n",
    "\n",
    "\n",
    "def down_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.MaxPooling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.MaxPooling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels * 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "def up_sampling_module(input_tensor):\n",
    "    channels = list(input_tensor.shape)[-1]\n",
    "    main_branch = layers.Conv2D(channels, kernel_size=(1, 1), activation=\"relu\")(\n",
    "        input_tensor\n",
    "    )\n",
    "    main_branch = layers.Conv2D(\n",
    "        channels, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"\n",
    "    )(main_branch)\n",
    "    main_branch = layers.UpSampling2D()(main_branch)\n",
    "    main_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(main_branch)\n",
    "    skip_branch = layers.UpSampling2D()(input_tensor)\n",
    "    skip_branch = layers.Conv2D(channels // 2, kernel_size=(1, 1))(skip_branch)\n",
    "    return layers.Add()([skip_branch, main_branch])\n",
    "\n",
    "\n",
    "# MRB Block\n",
    "def multi_scale_residual_block(input_tensor, channels):\n",
    "    # features\n",
    "    level1 = input_tensor\n",
    "    level2 = down_sampling_module(input_tensor)\n",
    "    level3 = down_sampling_module(level2)\n",
    "    # DAU\n",
    "    level1_dau = dual_attention_unit_block(level1)\n",
    "    level2_dau = dual_attention_unit_block(level2)\n",
    "    level3_dau = dual_attention_unit_block(level3)\n",
    "    # SKFF\n",
    "    level1_skff = selective_kernel_feature_fusion(\n",
    "        level1_dau,\n",
    "        up_sampling_module(level2_dau),\n",
    "        up_sampling_module(up_sampling_module(level3_dau)),\n",
    "    )\n",
    "    level2_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(level1_dau), level2_dau, up_sampling_module(level3_dau)\n",
    "    )\n",
    "    level3_skff = selective_kernel_feature_fusion(\n",
    "        down_sampling_module(down_sampling_module(level1_dau)),\n",
    "        down_sampling_module(level2_dau),\n",
    "        level3_dau,\n",
    "    )\n",
    "    # DAU 2\n",
    "    level1_dau_2 = dual_attention_unit_block(level1_skff)\n",
    "    level2_dau_2 = up_sampling_module((dual_attention_unit_block(level2_skff)))\n",
    "    level3_dau_2 = up_sampling_module(\n",
    "        up_sampling_module(dual_attention_unit_block(level3_skff))\n",
    "    )\n",
    "    # SKFF 2\n",
    "    skff_ = selective_kernel_feature_fusion(level1_dau_2, level3_dau_2, level3_dau_2)\n",
    "    conv = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(skff_)\n",
    "    return layers.Add()([input_tensor, conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_residual_group(input_tensor, num_mrb, channels):\n",
    "    conv1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_mrb):\n",
    "        conv1 = multi_scale_residual_block(conv1, channels)\n",
    "    conv2 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(conv1)\n",
    "    return layers.Add()([conv2, input_tensor])\n",
    "\n",
    "\n",
    "def mirnet_model(num_rrg, num_mrb, channels):\n",
    "    input_tensor = keras.Input(shape=[None, None, 3])\n",
    "    x1 = layers.Conv2D(channels, kernel_size=(3, 3), padding=\"same\")(input_tensor)\n",
    "    for _ in range(num_rrg):\n",
    "        x1 = recursive_residual_group(x1, num_mrb, channels)\n",
    "    conv = layers.Conv2D(3, kernel_size=(3, 3), padding=\"same\")(x1)\n",
    "    output_tensor = layers.Add()([input_tensor, conv])\n",
    "    return keras.Model(input_tensor, output_tensor)\n",
    "\n",
    "\n",
    "model = mirnet_model(num_rrg=3, num_mrb=2, channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 3552s 47s/step - loss: 0.2055 - peak_signal_noise_ratio: 62.1991 - val_loss: 0.1325 - val_peak_signal_noise_ratio: 65.3302 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 3687s 49s/step - loss: 0.1740 - peak_signal_noise_ratio: 63.5086 - val_loss: 0.1247 - val_peak_signal_noise_ratio: 65.7425 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "36/75 [=============>................] - ETA: 31:19 - loss: 0.1628 - peak_signal_noise_ratio: 63.9532"
     ]
    }
   ],
   "source": [
    "def charbonnier_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.sqrt(tf.square(y_true - y_pred) + tf.square(1e-3)))\n",
    "\n",
    "\n",
    "def peak_signal_noise_ratio(y_true, y_pred):\n",
    "    return tf.image.psnr(y_pred, y_true, max_val=255.0)\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=charbonnier_loss, metrics=[peak_signal_noise_ratio]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_peak_signal_noise_ratio\",\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            min_delta=1e-7,\n",
    "            mode=\"max\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"peak_signal_noise_ratio\"], label=\"train_psnr\")\n",
    "plt.plot(history.history[\"val_peak_signal_noise_ratio\"], label=\"val_psnr\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.title(\"Train and Validation PSNR Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, titles, figure_size=(12, 12)):\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(len(images)):\n",
    "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
    "        _ = plt.imshow(images[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def infer(original_image):\n",
    "    image = keras.preprocessing.image.img_to_array(original_image)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    output = model.predict(image)\n",
    "    output_image = output[0] * 255.0\n",
    "    output_image = output_image.clip(0, 255)\n",
    "    output_image = output_image.reshape(\n",
    "        (np.shape(output_image)[0], np.shape(output_image)[1], 3)\n",
    "    )\n",
    "    output_image = Image.fromarray(np.uint8(output_image))\n",
    "    original_image = Image.fromarray(np.uint8(original_image))\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for low_light_image in random.sample(test_low_light_images, 6):\n",
    "    original_image = Image.open(low_light_image)\n",
    "    enhanced_image = infer(original_image)\n",
    "    plot_results(\n",
    "        [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
    "        [\"Original\", \"PIL Autocontrast\", \"MIRNet Enhanced\"],\n",
    "        (20, 12),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
